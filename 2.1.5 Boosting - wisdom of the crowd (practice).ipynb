{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting - Wisdom of the Crowd (practice)\n",
    "**This chapter includes**:\n",
    "- <a href=\"#data-preparation\">Data preparation</a>\n",
    "- <a href=\"#sdt\">Using single decision tree</a>\n",
    "- <a href=\"#ada\">Boosting with AdaBoost</a>\n",
    "- <a href=\"#gbt\">Gradient Boosted Trees - why not?</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's time to see how boosting is applied in practice. Hopefully the `scikit-learn` package provides all described packages. Begin with importing all required libraries. XGBoost package will be described more in later lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenneth\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# reproducibility\n",
    "seed = 104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data <a name='data-preparation' />\n",
    "In all examples we will be dealing with **binary classification**.  Generate 20 dimensional artificial dataset with 1000 samples, where 8 features holding information, 3 are redundant and 2 repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=8, n_redundant=3, n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally perform a split into train/test parts. It will be useful for validating the performance of all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All algorithms won't be tuned at this point. A sensible set of default settings will be applied, making the whole things less complicated.\n",
    "\n",
    "> [*Logarithmic loss*](https://www.kaggle.com/wiki/LogarithmicLoss) and accuracy were chosen to evaluate the results. It's also important to remeber about reproducibility - you should always set all `seed` parameters to the same value.\n",
    "\n",
    "Let's perform a target variable distribution sanity check before digging into algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      "Counter({1: 404, 0: 396})\n",
      "\n",
      "Test label distribution:\n",
      "Counter({0: 106, 1: 94})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label distribution:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable is equally distribued across both dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Decision Tree <a name='sdt' />\n",
    "The following code will create a single decision tree, fit it using training data and evaluate the results using test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Decision Tree ==\n",
      "Accuracy: 0.85\n",
      "Log loss: 5.01\n",
      "Number of nodes created: 143\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=seed)\n",
    "\n",
    "# train classifier\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# predict output\n",
    "decision_tree_y_pred  = decision_tree.predict(X_test)\n",
    "decision_tree_y_pred_prob  = decision_tree.predict_proba(X_test)\n",
    "\n",
    "# evaluation\n",
    "decision_tree_accuracy = accuracy_score(y_test, decision_tree_y_pred)\n",
    "decision_tree_logloss = log_loss(y_test, decision_tree_y_pred_prob)\n",
    "\n",
    "print(\"== Decision Tree ==\")\n",
    "print(\"Accuracy: {0:.2f}\".format(decision_tree_accuracy))\n",
    "print(\"Log loss: {0:.2f}\".format(decision_tree_logloss))\n",
    "print(\"Number of nodes created: {}\".format(decision_tree.tree_.node_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two things:\n",
    "\n",
    "1. the log loss score is not very promising (due to the fact that leaves in decision tree outputs either `0` or `1` as probability which is heaviliy penalized in case of errors, but the accuracy score is quite decent,\n",
    "2. the tree is complicated (large number of nodes)\n",
    "\n",
    "You can inspect first few predicted outputs, and see that only 2 instances out of 5 were classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:\n",
      "[1 1 0 0 0]\n",
      "\n",
      "Predicted labels:\n",
      "[1 1 0 0 0]\n",
      "\n",
      "Predicted probabilities:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('True labels:')\n",
    "print(y_test[:5,])\n",
    "print('\\nPredicted labels:')\n",
    "print(decision_tree_y_pred[:5,])\n",
    "print('\\nPredicted probabilities:')\n",
    "print(decision_tree_y_pred_prob[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-291bb6bf066a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# convert to PNG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"dot\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-Tpng\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_viz_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-o\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_png_file\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# display image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-l\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \"\"\"\n\u001b[1;32m--> 266\u001b[1;33m     \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"args\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-l\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[0;32m    674\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    955\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    958\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "dt_viz_file = 'D:/kenneth/VG/dt.dot'\n",
    "dt_png_file = 'D:/kenneth/VG/dt.png'\n",
    "\n",
    "# create visualization\n",
    "export_graphviz(decision_tree, out_file=dt_viz_file)\n",
    "\n",
    "# convert to PNG\n",
    "command = [\"dot\", \"-Tpng\", dt_viz_file, \"-o\", dt_png_file]\n",
    "subprocess.check_call(command)\n",
    "\n",
    "# display image\n",
    "Image(filename=dt_png_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost <a name='ada' />\n",
    "In the example below we are creating a AdaBoost classifier running on 1000 iterations (1000 trees created). Also we are growing decision node up to first split (they are called *decision stumps*). We are also going to use `SAMME` algorithm which is inteneded to work with discrete data (output from `base_estimator` is `0` or `1`). Please refer to the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) and [here](http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    algorithm='SAMME',\n",
    "    n_estimators=1000,\n",
    "    random_state=seed)\n",
    "\n",
    "# train classifier\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# calculate predictions\n",
    "adaboost_y_pred = adaboost.predict(X_test)\n",
    "adaboost_y_pred_prob = adaboost.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_y_pred)\n",
    "adaboost_logloss = log_loss(y_test, adaboost_y_pred_prob)\n",
    "\n",
    "print(\"== AdaBoost ==\")\n",
    "print(\"Accuracy: {0:.2f}\".format(adaboost_accuracy))\n",
    "print(\"Log loss: {0:.2f}\".format(adaboost_logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss metrics is much lower than in single decision tree (mainly to the fact that now we obtain probabilities output). The accuracy is the same, but notice that the structure of the tree is much simpler. We are creating 1000 **decision tree stumps**.\n",
    "\n",
    "Also here a quick peek into predicted values show that now 4 out of 5 first test instances are classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True labels:')\n",
    "print(y_test[:5,])\n",
    "print('\\nPredicted labels:')\n",
    "print(adaboost_y_pred[:5,])\n",
    "print('\\nPredicted probabilities:')\n",
    "print(adaboost_y_pred_prob[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for clarity, let's check how the first tree looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_t1 = adaboost.estimators_[0]\n",
    "ada_t1_viz_file = '../images/ada-t1.dot'\n",
    "ada_t1_png_file = '../images/ada-t1.png'\n",
    "\n",
    "# create visualization\n",
    "export_graphviz(ada_t1, out_file=ada_t1_viz_file)\n",
    "\n",
    "# convert to PNG\n",
    "command = [\"dot\", \"-Tpng\", ada_t1_viz_file, \"-o\", ada_t1_png_file]\n",
    "subprocess.check_call(command)\n",
    "\n",
    "# display image\n",
    "Image(filename=ada_t1_png_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's it's error and contribution into final ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error: {0:.2f}\".format(adaboost.estimator_errors_[0]))\n",
    "print(\"Tree importance: {0:.2f}\".format(adaboost.estimator_weights_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees<a name='gbt' />\n",
    "Let's construct a gradient boosted tree consiting of 1000 trees where each successive one will be created with gradient optimization. Again we are going to leave most parameters with their default values, specifiy only maximum depth of the tree to 1 (again decision stumps), and setting warm start for more intelligent computations. Please refer to the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) if something is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(\n",
    "    max_depth=1,\n",
    "    n_estimators=1000,\n",
    "    warm_start=True,\n",
    "    random_state=seed)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "gbc_y_pred = gbc.predict(X_test)\n",
    "gbc_y_pred_prob = gbc.predict_proba(X_test)\n",
    "\n",
    "# calculate log loss\n",
    "gbc_accuracy = accuracy_score(y_test, gbc_y_pred)\n",
    "gbc_logloss = log_loss(y_test, gbc_y_pred_prob)\n",
    "\n",
    "print(\"== Gradient Boosting ==\")\n",
    "print(\"Accuracy: {0:.2f}\".format(gbc_accuracy))\n",
    "print(\"Log loss: {0:.2f}\".format(gbc_logloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained results are obviously the best of all presented algorithm. We have obtained most accurate algorithm giving more sensible predictions about class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True labels:')\n",
    "print(y_test[:5,])\n",
    "print('\\nPredicted labels:')\n",
    "print(gbc_y_pred[:5,])\n",
    "print('\\nPredicted probabilities:')\n",
    "print(gbc_y_pred_prob[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that GBC uses `DecisionTreeRegressor` classifier as the estimator with *mean-square error* as criterion. This results of slightly different output of the tree - now the leaf contains a predicted value (while the first splitting point remains the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_t1 = gbc.estimators_[2][0]\n",
    "gbc_t1_viz_file = '../images/gbc-t1.dot'\n",
    "gbc_t1_png_file = '../images/gbc-t1.png'\n",
    "\n",
    "# create visualization\n",
    "export_graphviz(gbc_t1, out_file=gbc_t1_viz_file)\n",
    "\n",
    "# convert to PNG\n",
    "command = [\"dot\", \"-Tpng\", gbc_t1_viz_file, \"-o\", gbc_t1_png_file]\n",
    "subprocess.check_call(command)\n",
    "\n",
    "# display image\n",
    "Image(filename=gbc_t1_png_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
